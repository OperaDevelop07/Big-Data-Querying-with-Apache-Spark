{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atmospheric-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 117,\n",
       " 'name': None,\n",
       " 'appId': None,\n",
       " 'owner': None,\n",
       " 'proxyUser': None,\n",
       " 'state': 'starting',\n",
       " 'kind': 'pyspark',\n",
       " 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None},\n",
       " 'log': ['stdout: ', '\\nstderr: ']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, pprint, requests, textwrap\n",
    "host = 'http://livyserveraddress'\n",
    "data = {'kind': 'pyspark'}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consecutive-treasurer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 117,\n",
       " 'name': None,\n",
       " 'appId': None,\n",
       " 'owner': None,\n",
       " 'proxyUser': None,\n",
       " 'state': 'starting',\n",
       " 'kind': 'pyspark',\n",
       " 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None},\n",
       " 'log': ['stdout: ', '\\nstderr: ']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_url = host + r.headers['location']\n",
    "r = requests.get(session_url, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "artistic-garage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'code': '\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import desc,asc\\nfrom pyspark.sql.functions import lower, col\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.functions import from_unixtime, hour, dayofyear\\nfrom pyspark.sql import Window\\n\\ndf_movies = (spark.read            .format(\\'csv\\')\\n            .option(\\'header\\',\\'true\\')\\n            .option(\\'delimiter\\',\",\")\\n            .option(\\'inferSchema\\',\\'true\\')\\n            .load(\"/home/administrator/Downloads/movielens/movie.csv\"))\\n\\n\\n\\n\\n\\ndf_rating = (spark.read            .format(\\'csv\\')\\n            .option(\\'header\\',\\'true\\')\\n            .option(\\'delimiter\\',\",\")\\n            .option(\\'inferSchema\\',\\'true\\')\\n            .load(\"/home/administrator/Downloads/movielens/rating.csv\"))\\n\\n\\n\\n\\n\\ndf_tag = (spark.read         .format(\\'csv\\')\\n         .option(\\'header\\',\\'true\\')\\n         .option(\\'delimiter\\',\",\")\\n         .option(\\'inferSchema\\',\\'true\\')\\n         .load(\"/home/administrator/Downloads/movielens/tag.csv\"))\\n\\n\\n\\n\\n\\ndf_genome_tags = (spark.read                 .format(\\'csv\\')\\n                 .option(\\'header\\',\\'true\\')\\n                 .option(\\'delimiter\\',\",\")\\n                 .option(\\'inferSchema\\',\\'true\\')\\n                 .load(\"/home/administrator/Downloads/movielens/genome_tags.csv\"))\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#codefile = open('C:\\livy_files\\load_libs_dataframes_livy.py','r')\n",
    "#code = codefile.read()\n",
    "#codefile.close()\n",
    "data = {\n",
    "        'kind' : 'pyspark',#'code' : textwrap.dedent(code)\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "       \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc,asc\n",
    "from pyspark.sql.functions import lower, col\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import from_unixtime, hour, dayofyear\n",
    "from pyspark.sql import Window\n",
    "  \n",
    "df_movies = (spark.read\\\n",
    "            .format('csv')\n",
    "            .option('header','true')\n",
    "            .option('delimiter',\",\")\n",
    "            .option('inferSchema','true')\n",
    "            .load(\"/home/administrator/Downloads/movielens/movie.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_rating = (spark.read\\\n",
    "            .format('csv')\n",
    "            .option('header','true')\n",
    "            .option('delimiter',\",\")\n",
    "            .option('inferSchema','true')\n",
    "            .load(\"/home/administrator/Downloads/movielens/rating.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_tag = (spark.read\\\n",
    "         .format('csv')\n",
    "         .option('header','true')\n",
    "         .option('delimiter',\",\")\n",
    "         .option('inferSchema','true')\n",
    "         .load(\"/home/administrator/Downloads/movielens/tag.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_genome_tags = (spark.read\\\n",
    "                 .format('csv')\n",
    "                 .option('header','true')\n",
    "                 .option('delimiter',\",\")\n",
    "                 .option('inferSchema','true')\n",
    "                 .load(\"/home/administrator/Downloads/movielens/genome_tags.csv\"))\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confident-exemption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'code': '\\njoinedq1 = df_movies.filter(df_movies.title.like(\"%Jumanji%\")).join(df_rating, df_rating[\"movieId\"] == df_movies[\"movieId\"], \"inner\").drop(df_rating.movieId).select(df_rating[\\'userId\\'],df_movies[\"title\"]).groupBy(df_rating.userId).count().agg(sum(col(\\'count\\')))\\njoinedq1.select(col(\\'sum(count)\\').alias(\\'Jumanji_viewers\\')).show()\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 1 executio,find jumanji viewers\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "       \n",
    "joinedq1 = df_movies.filter(df_movies.title.like(\"%Jumanji%\")).join(df_rating, df_rating[\"movieId\"] == df_movies[\"movieId\"], \"inner\").drop(df_rating.movieId).select(df_rating['userId'],df_movies[\"title\"]).groupBy(df_rating.userId).count().agg(sum(col('count')))\n",
    "joinedq1.select(col('sum(count)').alias('Jumanji_viewers')).show()\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broke-palestine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'code': 'df_low=df_tag.withColumn(\"tag\", lower(col(\"tag\")))      \\ndf_low_clr = df_low.select(col(\\'userId\\'),col(\\'movieId\\'),regexp_replace(df_low.tag,\\'[^a-zA-Z0-9]\\',\"\").alias(\\'tag_clr\\'))\\njoinq2 = df_movies.join(df_low_clr,df_movies[\\'movieId\\'] == df_low_clr[\\'movieId\\'],\\'inner\\').drop(df_movies.movieId).filter(df_low_clr.tag_clr.like(\\'boring\\')).dropDuplicates(subset=[\\'movieId\\']).select(col(\\'userId\\'),col(\\'movieId\\'),col(\\'title\\'),col(\\'tag_clr\\').alias(\\'tag\\')).orderBy(col(\\'title\\').asc()).show(truncate=False)\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 2 \n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "df_low=df_tag.withColumn(\"tag\", lower(col(\"tag\")))      \n",
    "df_low_clr = df_low.select(col('userId'),col('movieId'),regexp_replace(df_low.tag,'[^a-zA-Z0-9]',\"\").alias('tag_clr'))\n",
    "joinq2 = df_movies.join(df_low_clr,df_movies['movieId'] == df_low_clr['movieId'],'inner').drop(df_movies.movieId).filter(df_low_clr.tag_clr.like('boring')).dropDuplicates(subset=['movieId']).select(col('userId'),col('movieId'),col('title'),col('tag_clr').alias('tag')).orderBy(col('title').asc()).show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complimentary-princeton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3,\n",
       " 'code': 'df3_ratings = df_rating.filter(df_rating.rating > 3).select(df_rating.userId,df_rating.rating)\\ndf_tagboll = df_low_clr.filter(df_low_clr.tag_clr.like(\\'bollywood\\')).select(df_low_clr.movieId,df_low_clr.userId,df_low_clr.tag_clr.alias(\\'tag\\'))\\n\\njoin3_ratag = (df3_ratings.join(df_tagboll,df3_ratings[\\'userId\\'] == df_tagboll[\\'userId\\'],\"inner\")                           .drop(df3_ratings.userId).select(df_tagboll.movieId,df_tagboll[\\'userId\\'].alias(\"userIdJoined\"),df_tagboll[\\'tag\\'],df_rating[\\'rating\\'])).dropDuplicates(subset = [\\'userIdJoined\\']).orderBy(col(\\'userIdJoined\\').asc()).show(truncate=False)\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 3\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "df3_ratings = df_rating.filter(df_rating.rating > 3).select(df_rating.userId,df_rating.rating)\n",
    "df_tagboll = df_low_clr.filter(df_low_clr.tag_clr.like('bollywood')).select(df_low_clr.movieId,df_low_clr.userId,df_low_clr.tag_clr.alias('tag'))\n",
    "\n",
    "join3_ratag = (df3_ratings.join(df_tagboll,df3_ratings['userId'] == df_tagboll['userId'],\"inner\")\\\n",
    "                           .drop(df3_ratings.userId).select(df_tagboll.movieId,df_tagboll['userId'].alias(\"userIdJoined\"),df_tagboll['tag'],df_rating['rating'])).dropDuplicates(subset = ['userIdJoined']).orderBy(col('userIdJoined').asc()).show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extensive-ready",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4,\n",
       " 'code': 'rating_years = df_rating.select(df_rating.movieId,year(df_rating.timestamp).alias(\\'rate_year\\'),df_rating.rating)\\nrating_years_q = rating_years.join(df_movies,rating_years[\\'movieId\\'] == df_movies[\\'movieId\\'],\\'inner\\').drop(df_movies.movieId).groupBy(col(\\'title\\'),col(\\'rate_year\\')).avg(\\'rating\\').withColumn(\"rating_rank\",row_number().over(Window.partitionBy(\"rate_year\").orderBy(col(\\'avg(rating)\\').desc()))).orderBy(col(\\'rate_year\\').asc()).filter(col(\\'rating_rank\\')<=10).withColumn(\"alphab_rank\",row_number().over(Window.partitionBy(\"rate_year\").orderBy(col(\\'title\\').asc())))#.dropDuplicates(subset=[\\'title\\'])\\nrating_years_q.show(truncate=False)\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 4\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "rating_years = df_rating.select(df_rating.movieId,year(df_rating.timestamp).alias('rate_year'),df_rating.rating)\n",
    "rating_years_q = rating_years.join(df_movies,rating_years['movieId'] == df_movies['movieId'],'inner').drop(df_movies.movieId).groupBy(col('title'),col('rate_year')).avg('rating').withColumn(\"rating_rank\",row_number().over(Window.partitionBy(\"rate_year\").orderBy(col('avg(rating)').desc()))).orderBy(col('rate_year').asc()).filter(col('rating_rank')<=10).withColumn(\"alphab_rank\",row_number().over(Window.partitionBy(\"rate_year\").orderBy(col('title').asc())))#.dropDuplicates(subset=['title'])\n",
    "rating_years_q.show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polish-peter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5,\n",
       " 'code': 'df_join5 =  df_movies.join(df_tag,df_movies[\\'movieId\\'] == df_tag[\\'movieId\\'],\\'inner\\').select(df_tag[\\'tag\\'],df_movies[\\'title\\'],df_tag.timestamp)\\ndf_join5_conc = df_join5.filter(year(df_join5.timestamp)==2015).groupBy(col(\"title\")).agg(collect_list(col(\"tag\")).alias(\\'tag\\')).withColumn(\"tag\", concat_ws(\",\", col(\"tag\"))).orderBy(col(\\'title\\').asc()).show(truncate=False)\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 5\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "df_join5 =  df_movies.join(df_tag,df_movies['movieId'] == df_tag['movieId'],'inner').select(df_tag['tag'],df_movies['title'],df_tag.timestamp)\n",
    "df_join5_conc = df_join5.filter(year(df_join5.timestamp)==2015).groupBy(col(\"title\")).agg(collect_list(col(\"tag\")).alias('tag')).withColumn(\"tag\", concat_ws(\",\", col(\"tag\"))).orderBy(col('title').asc()).show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nutritional-seventh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6,\n",
       " 'code': 'join6q = df_movies.join(df_rating, df_movies[\\'movieId\\'] == df_rating[\\'movieId\\'],\"inner\").drop(df_rating.movieId).groupBy(df_movies.title).count().orderBy(col(\"count\").desc()).show(truncate=False)\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 6\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "join6q = df_movies.join(df_rating, df_movies['movieId'] == df_rating['movieId'],\"inner\").drop(df_rating.movieId).groupBy(df_movies.title).count().orderBy(col(\"count\").desc()).show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "british-clark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 7,\n",
       " 'code': \"count_ratings = df_rating.groupBy(col('userId'),year(df_rating.timestamp)).count().select(col('userId'),col('year(timestamp)').alias('rating_year'),col('count').cast('int').alias('rating_count')).withColumn('count_rank',row_number().over(Window.partitionBy('rating_year').orderBy(col('rating_count').desc()))).orderBy(col('rating_year').asc()).filter(col('count_rank')<=10).withColumn('id_alphab_rank',row_number().over(Window.partitionBy('rating_year').orderBy(col('userId').asc())))\\ncount_ratings.show(truncate=False)\\n\",\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 7\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "count_ratings = df_rating.groupBy(col('userId'),year(df_rating.timestamp)).count().select(col('userId'),col('year(timestamp)').alias('rating_year'),col('count').cast('int').alias('rating_count')).withColumn('count_rank',row_number().over(Window.partitionBy('rating_year').orderBy(col('rating_count').desc()))).orderBy(col('rating_year').asc()).filter(col('count_rank')<=10).withColumn('id_alphab_rank',row_number().over(Window.partitionBy('rating_year').orderBy(col('userId').asc())))\n",
    "count_ratings.show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dated-significance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 8,\n",
       " 'code': 'movies_genres = df_movies.withColumn(\"genres\",explode(split(\"genres\",\"[|]\")))\\ngenres_nodupes = movies_genres.dropDuplicates(subset=[\\'movieId\\']).filter(~col(\\'genres\\').like(\\'%(no genres listed)%\\')).orderBy(col(\\'movieId\\').asc())\\nrating_group = df_rating.groupBy(df_rating.movieId).count()\\n\\nquer8_join = genres_nodupes.join(rating_group,rating_group[\\'movieId\\'] == genres_nodupes[\\'movieId\\'],\"inner\").drop(genres_nodupes.movieId).dropDuplicates(subset=[\\'movieId\\']).withColumn(\"count_rtng_rank\",row_number().over(Window.partitionBy(\"genres\").orderBy(col(\\'count\\').desc()))).filter(col(\\'count_rtng_rank\\')==1).orderBy(col(\"genres\").asc())\\nquer8_join.show(truncate=False)\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 8\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "movies_genres = df_movies.withColumn(\"genres\",explode(split(\"genres\",\"[|]\")))\n",
    "genres_nodupes = movies_genres.dropDuplicates(subset=['movieId']).filter(~col('genres').like('%(no genres listed)%')).orderBy(col('movieId').asc())\n",
    "rating_group = df_rating.groupBy(df_rating.movieId).count()\n",
    "\n",
    "quer8_join = genres_nodupes.join(rating_group,rating_group['movieId'] == genres_nodupes['movieId'],\"inner\").drop(genres_nodupes.movieId).dropDuplicates(subset=['movieId']).withColumn(\"count_rtng_rank\",row_number().over(Window.partitionBy(\"genres\").orderBy(col('count').desc()))).filter(col('count_rtng_rank')==1).orderBy(col(\"genres\").asc())\n",
    "quer8_join.show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pregnant-oasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 9,\n",
       " 'code': \"df_hour_day = df_rating.select(col('movieId'),col('userId'),regexp_extract(df_rating.timestamp,'\\\\d\\\\d\\\\d\\\\d\\\\-\\\\d\\\\d\\\\-\\\\d\\\\d\\\\s\\\\d\\\\d',0).alias('day_hour')).groupBy(col('movieId'),col('day_hour')).count().filter(col('count')>1).agg(sum(col('count'))).show()\\n\",\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 9\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "df_hour_day = df_rating.select(col('movieId'),col('userId'),regexp_extract(df_rating.timestamp,'\\d\\d\\d\\d\\-\\\\d\\d\\-\\\\d\\d\\\\s\\d\\d',0).alias('day_hour')).groupBy(col('movieId'),col('day_hour')).count().filter(col('count')>1).agg(sum(col('count'))).show()\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "painful-graduation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10,\n",
       " 'code': 'join_movies10 = df_low_clr.join(genres_nodupes,df_low_clr[\\'movieId\\']  == genres_nodupes[\\'movieId\\']).drop(genres_nodupes.movieId)\\ndf_q10 = df_rating.join(join_movies10,df_rating[\\'movieId\\'] == join_movies10[\\'movieId\\'],\\'inner\\').drop(df_rating.movieId).filter(col(\\'rating\\')>3.5).filter(join_movies10.tag_clr.like(\"funny\")).dropDuplicates(subset=[\\'movieId\\']).groupBy(col(\\'genres\\')).count().orderBy(col(\\'genres\\').asc())\\ndf_q10.show(truncate=False)\\n',\n",
       " 'state': 'waiting',\n",
       " 'output': None,\n",
       " 'progress': 0.0,\n",
       " 'started': 0,\n",
       " 'completed': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query 10\n",
    "data = {\n",
    "        'kind' : 'pyspark',\n",
    "        'code' : textwrap.dedent(\"\"\"\\\n",
    "join_movies10 = df_low_clr.join(genres_nodupes,df_low_clr['movieId']  == genres_nodupes['movieId']).drop(genres_nodupes.movieId)\n",
    "df_q10 = df_rating.join(join_movies10,df_rating['movieId'] == join_movies10['movieId'],'inner').drop(df_rating.movieId).filter(col('rating')>3.5).filter(join_movies10.tag_clr.like(\"funny\")).dropDuplicates(subset=['movieId']).groupBy(col('genres')).count().orderBy(col('genres').asc())\n",
    "df_q10.show(truncate=False)\n",
    "        \"\"\")   \n",
    "}\n",
    "\n",
    "\n",
    "#session_url = host + '/sessions/110'#instead of the last number here in the /sessions/... u can put ur session id to have the correct connection\n",
    "\n",
    "\n",
    "statement_url = session_url+'/statements'\n",
    "r = requests.post(statement_url, data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "democratic-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.delete(session_url, headers=headers)#close livy session\n",
    "pprint.pprint(r.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
